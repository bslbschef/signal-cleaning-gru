为何存储模型的文件夹名为model_checkpoints，这里checkpoints是什么概念？

`model_checkpoints` 是一个常见的命名约定，用于存储模型在训练过程中的检查点（checkpoints）。检查点是指在训练过程中保存的模型状态，通常包括模型的权重（weights）和优化器的状态（optimizer state）。以下是关于 `checkpoints` 的详细解释：

### 检查点（Checkpoints）的概念

1. **保存模型状态**:
   - **模型权重**: 模型在训练过程中学习到的参数。
   - **优化器状态**: 优化器（如 Adam、SGD）的状态信息，包括学习率、动量等。

2. **用途**:
   - **恢复训练**: 如果训练过程中断，可以从最近的检查点恢复训练，避免从头开始。
   - **模型选择**: 在 K 折交叉验证或其他验证过程中，保存每个折的最佳模型。
   - **模型评估**: 保存不同训练阶段的模型，用于后续评估和比较。

3. **命名约定**:
   - **`model_checkpoints`**: 常用于存储模型的检查点文件。
   - **`checkpoints`**: 也可以单独使用，表示检查点文件夹。
   - **`saved_models`**: 用于存储最终训练好的模型。
   - **`runs`**: 用于存储不同实验的运行结果。

### 为什么使用 `checkpoints`?

1. **恢复训练**:
   - 如果训练过程中断（例如，由于硬件故障或需要调整超参数），可以从最近的检查点恢复训练，而不是从头开始。
   - 这可以节省大量的时间和计算资源。

2. **模型选择**:
   - 在 K 折交叉验证中，每个折可能会有不同的最佳模型。通过保存每个折的最佳检查点，可以在验证结束后选择最佳模型。
   - 这有助于确保最终选择的模型在不同数据子集上表现良好。

3. **模型评估**:
   - 保存不同训练阶段的检查点，可以用于后续评估和比较。
   - 例如，可以评估每个检查点在验证集上的性能，选择最佳的检查点作为最终模型。

4. **实验管理**:
   - 使用 `checkpoints` 文件夹可以更好地管理不同实验的模型状态。
   - 例如，可以为每个实验创建一个单独的 `checkpoints` 文件夹，便于区分和管理。

### 示例

假设您在训练过程中保存了多个检查点，文件夹结构可能如下：

```
model_checkpoints/
├── best_mlp_fold_1.pth
├── best_mlp_fold_2.pth
├── best_mlp_fold_3.pth
├── best_mlp_fold_4.pth
├── best_mlp_fold_5.pth
├── best_lstm_fold_1.pth
├── best_lstm_fold_2.pth
├── best_lstm_fold_3.pth
├── best_lstm_fold_4.pth
├── best_lstm_fold_5.pth
└── ...
```


### 在代码中的应用

在您的代码中，`model_checkpoints` 文件夹用于存储每个折的最佳模型检查点。以下是相关代码片段：

```python
# 参数解析
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, required=False, choices=['mlp', 'lstm', 'gru', 'transformer'], default='mlp')
    parser.add_argument('--input_dir', type=str, default='./data_train/inputs')
    parser.add_argument('--label_dir', type=str, default='./data_train/labels')
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--hidden_size', type=int, default=128)
    parser.add_argument('--nhead', type=int, default=4)
    parser.add_argument('--num_layers', type=int, default=2)
    parser.add_argument('--alpha', type=float, default=0.7)
    parser.add_argument('--max_seq_len', type=int, default=20*60*1)
    parser.add_argument('--k_folds', type=int, default=5)  # 添加 K 折交叉验证的参数
    parser.add_argument('--model_save_dir', type=str, default='./model_checkpoints')  # 添加模型存储地址的参数
    return parser.parse_args()
```


在 `train` 函数中，模型保存路径的生成如下：

```python
def train(model, train_loader, val_loader, args, fold):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    criterion = HybridLoss(args.alpha)

    best_val_loss = float('inf')
    for epoch in range(args.epochs):
        model.train()
        for inputs, labels, lengths in train_loader:
            inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)

            # 生成mask
            batch_size, seq_len = inputs.shape[:2]
            mask = torch.zeros(batch_size, seq_len, device=device)
            for i in range(batch_size):
                mask[i, :lengths[i]] = 1

            optimizer.zero_grad()

            # 模型前向
            if isinstance(model, Transformer):
                src_key_padding_mask = (mask == 0)
                outputs = model(inputs, src_key_padding_mask=src_key_padding_mask)
            elif isinstance(model, (LSTM, GRU)):
                outputs = model(inputs, lengths)
            else:
                outputs = model(inputs)

            # 计算损失
            loss = criterion(outputs, labels, mask)
            loss.backward()
            optimizer.step()

        # 验证
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for inputs, labels, lengths in val_loader:
                inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)

                batch_size, seq_len = inputs.shape[:2]
                mask = torch.zeros(batch_size, seq_len, device=device)
                for i in range(batch_size):
                    mask[i, :lengths[i]] = 1

                if isinstance(model, Transformer):
                    src_key_padding_mask = (mask == 0)
                    outputs = model(inputs, src_key_padding_mask=src_key_padding_mask)
                elif isinstance(model, (LSTM, GRU)):
                    outputs = model(inputs, lengths)
                else:
                    outputs = model(inputs)

                val_loss += criterion(outputs, labels, mask).item()

        avg_val_loss = val_loss / len(val_loader)
        print(f'Epoch {epoch+1:03d} | Val Loss: {avg_val_loss:.4f}')

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            model_save_path = os.path.join(args.model_save_dir, f'best_{args.model}_fold_{fold+1}.pth')
            os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
            torch.save(model.state_dict(), model_save_path)

    return best_val_loss
```


### 具体解释

1. **参数解析**:
   ```python
   parser.add_argument('--model_save_dir', type=str, default='./model_checkpoints')
   ```

   - 添加 `--model_save_dir` 参数，用于指定模型存储的目录，默认为 `./model_checkpoints`。

2. **确保目录存在**:
   ```python
   os.makedirs(args.model_save_dir, exist_ok=True)
   ```

   - 在训练之前确保指定的目录存在。如果目录不存在，会自动创建。

3. **保存模型**:
   ```python
   model_save_path = os.path.join(args.model_save_dir, f'best_{args.model}_fold_{fold+1}.pth')
   os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
   torch.save(model.state_dict(), model_save_path)
   ```

   - 生成模型保存路径，使用 `args.model_save_dir` 和 `fold` 参数。
   - 确保保存路径的目录存在。
   - 保存模型的状态字典到指定路径。

### 总结

- **检查点（Checkpoints）**: 保存模型在训练过程中的状态，包括模型权重和优化器状态。
- **用途**:
  - **恢复训练**: 从最近的检查点恢复训练。
  - **模型选择**: 保存每个折的最佳模型。
  - **模型评估**: 保存不同训练阶段的模型，用于后续评估和比较。
- **命名约定**:
  - **`model_checkpoints`**: 常用于存储模型的检查点文件。
  - **`checkpoints`**: 也可以单独使用，表示检查点文件夹。
  - **`saved_models`**: 用于存储最终训练好的模型。
  - **`runs`**: 用于存储不同实验的运行结果。

通过使用 `checkpoints` 文件夹，可以更好地管理模型的训练过程，确保模型的状态在训练过程中得到妥善保存和管理。